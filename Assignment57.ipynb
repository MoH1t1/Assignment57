{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Q1. Relationship between Polynomial Functions and Kernel Functions in Machine Learning\n\nPolynomial functions and kernel functions are related in that a polynomial kernel is a specific type of kernel function used in kernel-based learning algorithms like Support Vector \nMachines (SVM).\n\nA kernel function computes the similarity between two data points in a transformed feature space without explicitly computing the transformation. This is known as the kernel trick.\nThe polynomial kernel is defined as K(x,y) = (x.y + c)^d  where x and y are feature vectors, c is a constant (also called the coefficient), and d is the degree of the polynomial.\n\nRelationship:\nPolynomial kernels allow SVM to learn non-linear decision boundaries by mapping data into a higher-dimensional space.\nFor example, in a 2D space, a degree-2 polynomial kernel would enable the classifier to draw a quadratic boundary.\n\n# Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n\nIn Support Vector Regression (SVR): Epsilon (ϵ) defines a margin of tolerance where predictions are not penalized. The model ignores errors within this margin.\n\nEffect:\nIncreasing ϵ makes the model tolerate larger deviations from the actual target, resulting in fewer support vectors.\nDecreasing ϵ reduces tolerance, leading to more support vectors and a more complex model.\n\n# Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n# and provide examples of when you might want to increase or decrease its value?\n\nEffect of Parameters on SVR Performance:\n    \nKernel Function: Defines the transformation of data into a higher-dimensional space. Example: Use RBF kernel for non-linear relationships.\n\nC Parameter: Controls the trade-off between minimizing error and maximizing the margin.\n             Low C: Larger margin, more tolerance for misclassified points (simple model).\n             High C: Smaller margin, less tolerance (complex model).\n\nEpsilon (ϵ): Defines the margin of tolerance for prediction errors.\n             Low ϵ: High sensitivity (complex model).\n             High ϵ: Low sensitivity (simpler model).\n                \nGamma Parameter: Defines how far the influence of a single data point reaches.\n                 Low γ: Generalizes better, smoother decision boundary.\n                 High γ: Fits tightly around data points (overfitting risk).\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX, y = make_classification(n_samples=200, n_features=2, n_classes=2, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nsvm_poly = SVC(kernel='poly', degree=3, coef0=1, C=1.0)\nsvm_poly.fit(X_train, y_train)\ny_pred = svm_poly.predict(X_test)\n\nprint(\"Accuracy with Polynomial Kernel:\", accuracy_score(y_test, y_pred))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Q5. Assignment:\n#  Import the necessary libraries and load the dataseg\n#  Split the dataset into training and testing setZ\n#  Preprocess the data using any technique of your choice (e.g. scaling, normalizationK\n#  Create an instance of the SVC classifier and train it on the training datW\n#  Use the trained classifier to predict the labels of the testing datW\n#  Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-scoreK\n#  Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to improve its performanc_\n#  Train the tuned classifier on the entire dataseg\n#  Save the trained classifier to a file for future use.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport joblib\n\ndata = pd.read_csv('your_dataset.csv')\n\nX = data.drop(columns=['target'])\ny = data['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nsvc = SVC()\n\nsvc.fit(X_train_scaled, y_train)\n\ny_pred = svc.predict(X_test_scaled)\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")\n\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'kernel': ['linear', 'rbf', 'poly'],\n    'gamma': ['scale', 'auto']\n}\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train_scaled, y_train)\n\nprint(\"Best parameters:\", grid_search.best_params_)\n\nbest_svc = grid_search.best_estimator_\nbest_svc.fit(scaler.fit_transform(X), y)\n\njoblib.dump(best_svc, 'svc_classifier.pkl')\nprint(\"Trained classifier saved as 'svc_classifier.pkl'\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}